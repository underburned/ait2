{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7182c113-d196-4a94-b168-ab98c2f22acd",
   "metadata": {},
   "source": [
    "# L1. Нейронные сети\n",
    "\n",
    "Источники:\n",
    " - [Нейронная сеть](https://ru.wikipedia.org/wiki/%D0%9D%D0%B5%D0%B9%D1%80%D0%BE%D0%BD%D0%BD%D0%B0%D1%8F_%D1%81%D0%B5%D1%82%D1%8C)\n",
    " - [Структура и принцип работы полносвязных нейронных сетей](https://proproprogs.ru/neural_network/struktura-i-princip-raboty-polnosvyaznyh-neyronnyh-setey), [YouTube](https://www.youtube.com/watch?v=VqChpNNYZ8Q&list=PLA0M1Bcd0w8yv0XGiF1wjerjSZVSrYbjh)\n",
    " - [Персептрон - возможности классификации образов, задача XOR](https://proproprogs.ru/neural_network/perseptron-vozmozhnosti-klassifikacii-obrazov-zadacha-xor), [YouTube](https://www.youtube.com/watch?v=t9QfcFNkG58&list=PLA0M1Bcd0w8yv0XGiF1wjerjSZVSrYbjh)\n",
    " - [Back propagation - алгоритм обучения по методу обратного распространения](https://proproprogs.ru/neural_network/back-propagation-algoritm-obucheniya-po-metodu-obratnogo-rasprostraneniya), [YouTube](https://www.youtube.com/watch?v=UXB9bFj-UA4&list=PLA0M1Bcd0w8yv0XGiF1wjerjSZVSrYbjh)\n",
    " - Тарик Рашид, \"Создаем нейронную сеть\"\n",
    "\n",
    "\n",
    "## Краткая история\n",
    "\n",
    " - 1943 г. Уоррен Мак-Каллок и Уолтер Питтс формализуют понятие нейронной сети в фундаментальной статье о логическом исчислении идей и нервной активности. Они выдвинули предположение, что нейроны можно упрощённо рассматривать как устройства, оперирующие двоичными числами, и назвали эту модель \"пороговой логикой\". Подобно своему биологическому прототипу нейроны Мак-Каллока-Питтса были способны обучаться путём подстройки параметров, описывающих синаптическую проводимость. Исследователи предложили конструкцию сети из электронных нейронов и показали, что подобная сеть может выполнять практически любые вообразимые числовые или логические операции. Мак-Каллок и Питтс предположили, что такая сеть в состоянии также обучаться, распознавать образы, обобщать, т.е. обладает всеми чертами интеллекта.\n",
    " - 1948 г. Опубликована книга Н. Винера о кибернетике. Основной идеей стало представление сложных биологических процессов математическими моделями.\n",
    " - 1949 г. Дональд Олден Хебб предлагает первый алгоритм обучения. Он первым предположил, что обучение заключается в первую очередь в изменениях силы синаптических связей. Теория Хебба считается типичным случаем самообучения, при котором испытуемая система спонтанно обучается выполнять поставленную задачу без вмешательства со стороны экспериментатора.\n",
    " - 1958 г. Фрэнк Розенблатт изобретает однослойный перцептрон и демонстрирует его способность решать задачи классификации. Перцептрон использовали для распознавания образов, прогнозирования погоды.\n",
    " - 1963 г. В Институте проблем передачи информации АН СССР Александром Павловичем Петровым проводится исследование задач, «трудных» для перцептрона.\n",
    " - 1969 г. Марвин Ли Минский публикует формальное доказательство ограниченности перцептрона и показывает, что он не способен решать некоторые задачи (проблема «чётности» и «один в блоке»), связанные с инвариантностью представлений. Второй важной проблемой было то, что компьютеры не обладали достаточной вычислительной мощностью, чтобы эффективно обрабатывать огромный объём вычислений, необходимый для больших нейронных сетей.\n",
    " - 1974 г. Пол Вербос и Александр Иванович Галушкин А. И. одновременно изобретают алгоритм обратного распространения ошибки для обучения многослойных перцептронов.\n",
    " - 1975 г. Кунихико Фукусима представляет когнитрон — самоорганизующуюся сеть, предназначенную для инвариантного распознавания образов, но это достигается только при помощи запоминания практически всех состояний образа.\n",
    " - 1982 г. Джон Джозеф Хопфилд показал, что нейронная сеть с обратными связями может представлять собой систему, минимизирующую энергию (сеть Хопфилда). Теуво Кохоненом представлены модели сети, обучающейся без учителя (нейронная сеть Кохонена), решающей задачи кластеризации, визуализации данных (самоорганизующаяся карта Кохонена) и другие задачи предварительного анализа данных.\n",
    " - 1986 г. Дэвидом Румельхартом, Джеффри Хинтоном и Рональдом Вильямсом, а также независимо и одновременно С.И. Барцевым и В.А. Охониным, развит метод обратного распространения ошибки.\n",
    " - 1989 г. Ян Лекун представил первую сверточную нейронную сеть (LeNet), способную распознавать рукописные цифры с хорошей скоростью и точностью распознавания. [Демонстрация работы](https://www.youtube.com/watch?v=FwFduRA_L6Q).\n",
    " - 2007 г. Джеффри Хинтоном в университете Торонто созданы алгоритмы глубокого обучения многослойных нейронных сетей. Хинтон при обучении нижних слоёв сети использовал ограниченную машину Больцмана (RBM — Restricted Boltzmann Machine). По Хинтону необходимо использовать много примеров распознаваемых образов (например, множество лиц людей на разных фонах). После обучения получается готовое быстро работающее приложение, способное решать конкретную задачу (например, осуществлять поиск лиц на изображении). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f448ad0d-ac81-4932-aa5f-b0d41548954f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Нейронная сеть\n",
    "\n",
    "<img width=\"600\" height=\"500\" alt=\"Структура нейрона\" src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/5/59/Neuron-rus.svg/1920px-Neuron-rus.svg.png\" style=\"margin:auto;\">\n",
    "<p style=\"text-align: center\">\n",
    "    Структура нейрона\n",
    "</p>\n",
    "\n",
    "Мозг состоит из большого количества нейронов, связанных между собой дендритами и аксонами. Дендриты - это отростки, по которым нервные импульсы передаются к телу нейрона. Эти отростки сильно ветвятся. У нейрона может быть несколько дендритов. Аксон - это отросток, по которому импульсы передаются от тела клетки.\n",
    "\n",
    "Фрэнк Розенблатт предложил перцептрон, схема которого вдохновлена биологическим прототипом:\n",
    "\n",
    "<img width=\"600\" alt=\"Перцептрон\" src=\"./data/images/ait2_l1/perceptron_1.svg\" style=\"margin:auto;\">\n",
    "<p style=\"text-align: center\">\n",
    "    Перцептрон\n",
    "</p>\n",
    "\n",
    "## Полносвязная нейронная сеть прямого распространения\n",
    "\n",
    "<img width=\"1000\" alt=\"Полносвязная нейронная сеть\" src=\"./data/images/ait2_l1/full_connected_network.svg\" style=\"margin:auto;\">\n",
    "<p style=\"text-align: center\">\n",
    "    Полносвязная нейронная сеть\n",
    "</p>\n",
    "\n",
    " - Каждый нейрон текущего слоя связан с каждым нейроном следующего слоя.\n",
    " - Прямое распространение - сигнал распространяется от входного слоя к выходному, не образуя обратных связей.  \n",
    "\n",
    "Каждая связь между нейронами имеет определенный вес $\\omega_{ij}$ - весовой коэффициент, связанный с сигналом, который передается от $i$-го нейрона текущего слоя к $j$-му нейрону следующего слоя:  \n",
    "\n",
    "<img width=\"1000\" alt=\"Полносвязная нейронная сеть\" src=\"./data/images/ait2_l1/full_connected_network_2.svg\" style=\"margin:auto;\">\n",
    "<p style=\"text-align: center\">\n",
    "    Полносвязная нейронная сеть\n",
    "</p>\n",
    "\n",
    "Сам по себе нейрон – это сумматор входных сигналов, который, затем, пропускает сумму через функцию $f(x)$, называемую функцией активации. Выходное значение этой функции и есть выходное значение нейрона.\n",
    "В концепции персептрона функции активации выбираются пороговыми:\n",
    "$$\n",
    "f(x) =\n",
    "  \\begin{cases}\n",
    "    a,  & \\quad \\text{если } x \\geq T\\\\\n",
    "    b,  & \\quad \\text{если } x < T\n",
    "  \\end{cases}\n",
    "$$\n",
    "\n",
    "### Пример\n",
    "\n",
    "Вы решили сходить в кино. Выбор вы осуществляете по трем параметрам:\n",
    " - наличие в фильме любимого актера;\n",
    " - интересное описание сюжета;\n",
    " - жанр фильма (вы недолюбливаете детективы, например).\n",
    "\n",
    "<img width=\"1000\" alt=\"Поход в кино\" src=\"./data/images/ait2_l1/cinema_1.svg\" style=\"margin:auto;\">\n",
    "<p style=\"text-align: center\">\n",
    "    Поход в кино\n",
    "</p>\n",
    "\n",
    "Сценарий 1:\n",
    "- [x] Любимый актер\n",
    "- [x] Интересный сюжет\n",
    "- [x] Детективный жанр  \n",
    "$y = f(0.5 \\cdot 1 + 0.5 \\cdot 1 - 0.5 \\cdot 1) = f(0.5) = 1$. Получаем граничное значение, ну, можно и сходить.\n",
    "\n",
    "Сценарий 2:\n",
    "- [x] Любимый актер\n",
    "- [ ] Интересный сюжет\n",
    "- [ ] Детективный жанр  \n",
    "$y = f(0.5 \\cdot 1 + 0.5 \\cdot 0 - 0.5 \\cdot 0) = f(0.5) = 1$. Любимый актер присутствует, сюжет не ахти, зато не нудный детектив. Тоже сойдет.\n",
    "\n",
    "Сценарий 3:\n",
    "- [x] Любимый актер\n",
    "- [ ] Интересный сюжет\n",
    "- [x] Детективный жанр  \n",
    "$y = f(0.5 \\cdot 1 + 0.5 \\cdot 0 - 0.5 \\cdot 1) = f(0) = 0$. Любимый актер присутствует, сюжета нет, да еще и нелюбимый жанр. Неа.\n",
    "\n",
    "Максимальная мотивация сходить в кино достигается при:\n",
    "- [x] Любимый актер\n",
    "- [x] Интересный сюжет\n",
    "- [ ] Детективный жанр  \n",
    "$y = f(0.5 \\cdot 1 + 0.5 \\cdot 1 - 0.5 \\cdot 0) = f(1) = 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a15c3c4-3a22-496a-99a7-2ba9dbd258ed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fav_actor</th>\n",
       "      <th>interesting_plot</th>\n",
       "      <th>detective_genre</th>\n",
       "      <th>go</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Кино ноуп</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Кино ноуп</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Го в кино</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Кино ноуп</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Го в кино</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Кино ноуп</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Го в кино</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Го в кино</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fav_actor  interesting_plot  detective_genre         go\n",
       "0          0                 0                0  Кино ноуп\n",
       "1          0                 0                1  Кино ноуп\n",
       "2          0                 1                0  Го в кино\n",
       "3          0                 1                1  Кино ноуп\n",
       "4          1                 0                0  Го в кино\n",
       "5          1                 0                1  Кино ноуп\n",
       "6          1                 1                0  Го в кино\n",
       "7          1                 1                1  Го в кино"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    " \n",
    "def act(x):\n",
    "    return 0 if x < 0.5 else 1\n",
    " \n",
    "def go(fav_actor, interesting_plot, detective_genre):\n",
    "    x = np.array([fav_actor, interesting_plot, detective_genre])\n",
    "    w = [0.5, 0.5, -0.5]\n",
    "    weight = np.array(w)\n",
    "    sum_end = np.dot(weight, x)\n",
    "    y = act(sum_end)\n",
    "    return y\n",
    "\n",
    "vars = np.array(list(itertools.chain(*itertools.product([0, 1], repeat=3))))\n",
    "vars = np.reshape(vars, (-1,3))\n",
    "\n",
    "df = pd.DataFrame(vars, columns = ['fav_actor', 'interesting_plot', 'detective_genre'])\n",
    "df['go'] = df.apply(lambda row : go(row['fav_actor'], row['interesting_plot'], row['detective_genre']), axis = 1)\n",
    "df['go'] = df['go'].astype(str)\n",
    "df['go'].replace({str(0): 'Кино ноуп', str(1): 'Го в кино'}, inplace = True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275c08de-42b0-4c84-84fa-9743631d2ce7",
   "metadata": {},
   "source": [
    "Пусть наличие любимого актера для нас - главное. Но и в целом, на интересное кино с ноунеймами без детективных арок сюжета мы готовы сходить. Добавим еще один слой.  \n",
    "\n",
    "<img width=\"1000\" alt=\"Поход в кино\" src=\"./data/images/ait2_l1/cinema_2.svg\" style=\"margin:auto;\">\n",
    "<p style=\"text-align: center\">\n",
    "    Поход в кино\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bcf2df6f-ef0c-4583-bbe4-cae69ae90c39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fav_actor</th>\n",
       "      <th>interesting_plot</th>\n",
       "      <th>detective_genre</th>\n",
       "      <th>go</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Кино ноуп</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Кино ноуп</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Го в кино</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Кино ноуп</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Го в кино</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Го в кино</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Го в кино</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Го в кино</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fav_actor  interesting_plot  detective_genre         go\n",
       "0          0                 0                0  Кино ноуп\n",
       "1          0                 0                1  Кино ноуп\n",
       "2          0                 1                0  Го в кино\n",
       "3          0                 1                1  Кино ноуп\n",
       "4          1                 0                0  Го в кино\n",
       "5          1                 0                1  Го в кино\n",
       "6          1                 1                0  Го в кино\n",
       "7          1                 1                1  Го в кино"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    " \n",
    "def act(x):\n",
    "    return 0 if x < 0.5 else 1\n",
    " \n",
    "def go(fav_actor, interesting_plot, detective_genre):\n",
    "    x = np.array([fav_actor, interesting_plot, detective_genre])\n",
    "    w11 = [1, 0.5, -0.4]\n",
    "    w12 = [0, 0.4, -0.5]\n",
    "    weight1 = np.array([w11, w12])   # матрица 2x3\n",
    "    weight2 = np.array([1, -1])      # вектор 1х3\n",
    " \n",
    "    sum_hidden = np.dot(weight1, x)  # вычисляем сумму на входах нейронов скрытого слоя\n",
    "    out_hidden = np.array([act(x) for x in sum_hidden])\n",
    " \n",
    "    sum_end = np.dot(weight2, out_hidden)\n",
    "    y = act(sum_end)\n",
    " \n",
    "    return y\n",
    "\n",
    "vars = np.array(list(itertools.chain(*itertools.product([0, 1], repeat=3))))\n",
    "vars = np.reshape(vars, (-1,3))\n",
    "\n",
    "df = pd.DataFrame(vars, columns = ['fav_actor', 'interesting_plot', 'detective_genre'])\n",
    "df['go'] = df.apply(lambda row : go(row['fav_actor'], row['interesting_plot'], row['detective_genre']), axis = 1)\n",
    "df['go'] = df['go'].astype(str)\n",
    "df['go'].replace({str(0): 'Кино ноуп', str(1): 'Го в кино'}, inplace = True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7013ea18-8df8-4874-b540-ae2c3565a2f1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Перцептрон\n",
    "\n",
    "Для простоты рассмотрим простейший перцептрон для задачи классификации двух классов образов, представленных двумя характеристиками $x_1, x_2$:\n",
    "\n",
    "<img width=\"500\" alt=\"Простейший перцептрон\" src=\"./data/images/ait2_l1/perceptron_2.svg\" style=\"margin:auto;\">\n",
    "<p style=\"text-align: center\">\n",
    "    Простейший перцептрон\n",
    "</p>\n",
    "\n",
    "С активационной функцией:\n",
    "$$f(x)=\\begin{cases}\n",
    "    1, &x\\geq 0 &\\rightarrow C_1\\\\\n",
    "    -1, &x < 0 &\\rightarrow C_2\n",
    "\\end{cases}$$\n",
    "\n",
    "То есть, если значение суммы больше или равно 0, то вектор принадлежит классу 1:\n",
    "$$[x_1, x_2]{^{\\mkern-1.5mu\\mathsf{T}}} \\in C_1$$\n",
    "Иначе классу 2:\n",
    "$$[x_1, x_2]{^{\\mkern-1.5mu\\mathsf{T}}} \\in C_2$$\n",
    "\n",
    "Далее, из вида активационной функции видно, что граница разделения двух классов проходит на уровне 0. То есть, если:\n",
    "$$f(x)=\\begin{cases}\n",
    "    \\omega_1 \\cdot x_1 + \\omega_2 \\cdot x_2 \\geq 0 &\\rightarrow C_1\\\\\n",
    "    \\omega_1 \\cdot x_1 + \\omega_2 \\cdot x_2 < 0 &\\rightarrow C_2\n",
    "\\end{cases}$$\n",
    "\n",
    "Значит, сумма:\n",
    "$$\\omega_1 \\cdot x_1 + \\omega_2 \\cdot x_2 = 0$$\n",
    "\n",
    "определяет границу разделения одного класса образов от другого. Ее еще можно записать в виде:\n",
    "$$x_2 = -\\frac{\\omega_1}{\\omega_2} \\cdot x_1$$\n",
    "\n",
    "То есть, это прямая с угловым коэффициентом\n",
    "$$k = - \\frac{\\omega_1}{\\omega_2}$$\n",
    "проходящая через начало системы координат:\n",
    "\n",
    "<img width=\"500\" alt=\"Разделяющая прямая\" src=\"./data/images/ait2_l1/perceptron_3.svg\" style=\"margin:auto;\">\n",
    "<p style=\"text-align: center\">\n",
    "    Разделяющая прямая\n",
    "</p>\n",
    "\n",
    "И все точки по одну сторону от этой прямой будут относиться к одному классу, а по другую сторону – к другому классу. Такая прямая получила название разделяющей прямой (в многомерном случае она превращается в гиперплоскость и называется разделяющей гиперплоскостью). Этот двумерный график хорошо демонстрирует возможность правильной классификации простейшим персептроном только линейно-разделимых образов.\n",
    "\n",
    "#### Пример.\n",
    "Пусть у нас есть два класса линейно-разделимых образов разделяющей прямой:\n",
    "$$x_2 = 1 \\cdot x_1$$\n",
    "\n",
    "В этом случае, для корректной классификации мы должны выбрать веса нейронной сети равными, но с противоположными знаками:\n",
    "$$k = 1 = - \\frac{\\omega_1}{\\omega_2} \\Rightarrow \\omega_1 = -0.5, \\omega_2 = 0.5$$\n",
    "\n",
    "Предположим, что все наши образы сдвигаются вверх по оси $x_2$ на некоторую величину:\n",
    "\n",
    "<img width=\"500\" alt=\"Разделяющая прямая\" src=\"./data/images/ait2_l1/perceptron_4.svg\" style=\"margin:auto;\">\n",
    "<p style=\"text-align: center\">\n",
    "    Разделяющая прямая\n",
    "</p>\n",
    "\n",
    "Теперь наша разделяющая прямая не сможет верно классифицировать такие образы, т.к. она проходит через начало координат. И как бы мы ее ни крутили, корректного разделения не получится. Необходимо смещение. Поэтому, в НС дополнительно определяют еще один вход для смещения разделяющей гиперплоскости. В английской литературе он называется `bias` (перевести можно как `порог`).\n",
    "\n",
    "<img width=\"750\" alt=\"Простейший перцептрон со смещением\" src=\"./data/images/ait2_l1/perceptron_5.svg\" style=\"margin:auto;\">\n",
    "<p style=\"text-align: center\">\n",
    "    Простейший перцептрон со смещением\n",
    "</p>\n",
    "\n",
    "С этим дополнительным входом, наша прямая принимает вид:\n",
    "$$x_2 = -\\frac{\\omega_1}{\\omega_2} \\cdot x_1 - \\frac{\\omega_3}{\\omega_2} \\cdot 1$$\n",
    "\n",
    "То есть, мы можем теперь сдвинуть ее на любое требуемое значение. Пусть все образы сдвинуты вверх по оси $x_2$ на величину $b$. Тогда третий весовой коэффициент НС следует выбрать из уравнения:\n",
    "$$-\\frac{\\omega_3}{\\omega_2} = b \\Rightarrow \\omega_3 = -b \\cdot \\omega_2$$\n",
    "\n",
    "### Задача XOR\n",
    "\n",
    "Рассмотренная нами НС с одним нейроном может классифицировать только линейно-разделимые образы. Однако, на практике чаще встречаются более сложные задачи. Например, представим, что классы наших образов распределены следующим образом:\n",
    "\n",
    "<img width=\"500\" alt=\"XOR 1\" src=\"./data/images/ait2_l1/perceptron_xor_1.svg\" style=\"margin:auto;\">\n",
    "\n",
    "Здесь невозможно провести одну линию для их правильной классификации. Как тогда быть? Например, провести две линии, вот так:\n",
    "\n",
    "<img width=\"500\" alt=\"XOR 2\" src=\"./data/images/ait2_l1/perceptron_xor_2.svg\" style=\"margin:auto;\">\n",
    "\n",
    "И все, что будет попадать между ними – отнесем к первому классу, а за их пределами – ко второму классу. Что это за НС, которая способна на такие операции? В действительности, все просто: каждая разделительная линия может быть представлена отдельным нейроном, а затем, результат их классификации объединяется результирующим нейроном выходного слоя:\n",
    "\n",
    "<img width=\"750\" alt=\"XOR 3\" src=\"./data/images/ait2_l1/perceptron_xor_3.svg\" style=\"margin:auto;\">\n",
    "\n",
    "Для простоты будем полагать, что на входы подаются только значения $0$ или $1$:\n",
    "$$x_1, x_2 \\in [0, 1]$$\n",
    "\n",
    "Тогда все наши образы будут лежать в углах квадрата:\n",
    "$$\n",
    "\\begin{matrix}\n",
    "  0 & 0 & C_2 \\\\\n",
    "  0 & 1 & C_1 \\\\\n",
    "  1 & 0 & C_1 \\\\\n",
    "  1 & 1 & C_2\n",
    " \\end{matrix}\n",
    "$$\n",
    "\n",
    "Если задать $C_2 = 0, C_1 = 1$, то получаем таблицу истинности для битовой операции `XOR` (`исключающее ИЛИ`). Поэтому в литературе задача разделения таких образов получила название `задачи XOR`.\n",
    "Активационная функция каждого нейрона будет иметь вид:\n",
    "$$\n",
    "f(x)=\\begin{cases}\n",
    "    1, & x \\geq 0\\\\\n",
    "    0, & x < 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Осталось определить веса связей НС для решения поставленной задачи классификации. Для начала, положим, что первый нейрон скрытого слоя будет формировать границу:\n",
    "$$x_2 = -1 \\cdot x_1 + 1.5$$\n",
    "\n",
    "Учитывая нашу формулу:\n",
    "$$x_2 = -\\frac{\\omega_1}{\\omega_2} \\cdot x_1 - \\frac{\\omega_3}{\\omega_2} \\cdot 1$$\n",
    "\n",
    "Веса входов первого нейрона для $x_1, x_2$ можно взять равными:\n",
    "$$\\omega_1 = \\omega_2 = 1$$\n",
    "\n",
    "а вес третьей связи:\n",
    "$$\\omega_3 = - b \\cdot \\omega_2 = - 1.5$$\n",
    "\n",
    "Получили прямую, которая формирует следующее разделение на плоскости:\n",
    "\n",
    "<img width=\"500\" alt=\"XOR 4\" src=\"./data/images/ait2_l1/perceptron_xor_4.svg\" style=\"margin:auto;\">\n",
    "\n",
    "Второй нейрон скрытого слоя будет формировать разделения прямой:\n",
    "$$x_2 = -1 \\cdot x_1 + 0.5$$\n",
    "\n",
    "и веса его связей можно взять равными:\n",
    "$$\\omega_1 = \\omega_2 = 1, \\omega_3 = -0.5$$\n",
    "\n",
    "Получаем следующую картину:\n",
    "\n",
    "<img width=\"500\" alt=\"XOR 5\" src=\"./data/images/ait2_l1/perceptron_xor_5.svg\" style=\"margin:auto;\">\n",
    "\n",
    "Теперь нам нужно объединить результаты их работы, чтобы получилась следующая разделяющая область:\n",
    "\n",
    "<img width=\"500\" alt=\"XOR 6\" src=\"./data/images/ait2_l1/perceptron_xor_6.svg\" style=\"margin:auto;\">\n",
    "\n",
    "Для этого из второго вычтем первое:\n",
    "\n",
    "<img width=\"1000\" alt=\"XOR 7\" src=\"./data/images/ait2_l1/perceptron_xor_7.svg\" style=\"margin:auto;\">\n",
    "\n",
    "Для надежности сместим эти значения на -0.5 и окончательно получим результат:\n",
    "\n",
    "<img width=\"500\" alt=\"XOR 8\" src=\"./data/images/ait2_l1/perceptron_xor_8.svg\" style=\"margin:auto;\">\n",
    "\n",
    "Веса в нашей НС будут следующими:\n",
    "\n",
    "<img width=\"750\" alt=\"XOR 9\" src=\"./data/images/ait2_l1/perceptron_xor_9.svg\" style=\"margin:auto;\">\n",
    "\n",
    "Как видим, результаты получаются именно такими, какие мы и ожидали, классификация задачи `XOR` выполнена успешно благодаря добавлению скрытого слоя нейронов.  \n",
    "\n",
    "Этот пример хорошо показывает, что добавляя новые нейроны, мы можем получать все более сложные формы разделяющих выпуклых областей, полученные комбинацией разделяющих линий или гиперплоскостей."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7beb649-a3e2-4250-9e6f-d53942a9fe05",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Back propagation - алгоритм обучения по методу обратного распространения\n",
    "\n",
    "При увеличении числа нейронов и связей ручной подбор весов НС становится попросту невозможным и возникает задача нахождения весовых коэффициентов связей НС. Этот процесс и называют обучением нейронной сети.  \n",
    "Один из распространенных подходов к обучению заключается в последовательном предъявлении НС векторов наблюдений и последующей корректировки весовых коэффициентов так, чтобы выходное значение совпадало с требуемым. Это называется `обучение с учителем`, так как для каждого вектора мы знаем нужный ответ и именно его требуем от нашей НС.  \n",
    "\n",
    "Теперь, главный вопрос: как построить алгоритм, который бы наилучшим образом находил весовые коэффициенты. Наилучший – это значит, максимально быстро и с максимально близкими выходными значениями для требуемых откликов. В общем случае эта задача не решена. Нет универсального алгоритма обучения. Поэтому, лучшее, что мы можем сделать – это выбрать тот алгоритм, который хорошо себя зарекомендовал в прошлом. Основной «рабочей лошадкой» здесь является алгоритм `back propagation` (`обратного распространения ошибки`), который, в свою очередь, базируется на алгоритме `градиентного спуска`.\n",
    "\n",
    "<img width=\"500\" alt=\"GD 1\" src=\"./data/images/ait2_l1/gd.gif\" style=\"margin:auto;\">  \n",
    "<img width=\"500\" alt=\"GD 1\" src=\"https://upload.wikimedia.org/wikipedia/commons/a/a3/Gradient_descent.gif\" style=\"margin:auto;\">\n",
    "\n",
    "Чтобы все лучше понять, предположим, что у нас имеется вот такая полносвязная НС прямого распространения с весами связей, выбранными произвольным образом в диапазоне от $[-0.5, 0.5]$. Здесь верхний индекс показывает принадлежность к тому или иному слою сети. Также, каждый нейрон имеет некоторую активационную функцию $f(x)$:\n",
    "\n",
    "<img width=\"750\" alt=\"GD 1\" src=\"./data/images/ait2_l1/back_prop_1.svg\" style=\"margin:auto;\">  \n",
    "\n",
    "На первом шаге делается прямой проход по сети. Мы пропускаем вектор наблюдения $[x_1, x_2]$ через эту сеть, и запоминаем все выходные значения нейронов скрытых слоев:\n",
    "\n",
    "$$\n",
    "\\begin{matrix}\n",
    "f_{11} = f(\\omega_{11}^{1} \\cdot x_1 + \\omega_{21}^{1} \\cdot x_2) \\\\\n",
    "f_{12} = f(\\omega_{12}^{1} \\cdot x_1 + \\omega_{22}^{1} \\cdot x_2) & f_{21} = f(\\omega_{11}^{2} \\cdot f_{11} + \\omega_{21}^{2} \\cdot f_{12} + \\omega_{31}^{2} \\cdot f_{13}) \\\\\n",
    "f_{13} = f(\\omega_{13}^{1} \\cdot x_1 + \\omega_{23}^{1} \\cdot x_2) & f_{22} = f(\\omega_{12}^{2} \\cdot f_{11} + \\omega_{22}^{2} \\cdot f_{12} + \\omega_{32}^{2} \\cdot f_{13}) \\\\\n",
    "\\end{matrix}\n",
    "$$\n",
    "\n",
    "и последнее выходное значение $y$:\n",
    "\n",
    "$$\n",
    "\\begin{matrix}\n",
    "v_{out} = \\omega_{11}^{3} \\cdot f_{21} + \\omega_{21}^{3} \\cdot f_{22} \\\\\n",
    "y = f(v_{out})\n",
    "\\end{matrix}\n",
    "$$\n",
    "\n",
    "Далее, мы знаем требуемый отклик $d$ для текущего вектора $[x_1, x_2]$, значит для него можно вычислить ошибку работы НС. Она будет равна:\n",
    "$$e = y - d$$\n",
    "\n",
    "Далее начинается самое главное – корректировка весов. Для этого делается обратный проход по НС: от последнего слоя – к первому.  \n",
    "\n",
    "Итак, у нас есть ошибка $e$ и некая функция активации нейронов $f(x)$. Первое, что нам нужно – это вычислить локальный градиент для выходного нейрона. Это делается по формуле:\n",
    "$$\\delta = e \\cdot f'(v_{out})$$\n",
    "\n",
    "Этот момент требует пояснения. Ранее используемая пороговая функция:\n",
    "$$\n",
    "f(x)=\\begin{cases}\n",
    "    1, & x \\geq 0.5\\\\\n",
    "    0, & x < 0.5\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "нам уже не подходит, т.к. она не дифференцируема на всем диапазоне значений $x$. Вместо этого для сетей с небольшим числом слоев, часто применяют или `гиперболический тангенс`:\n",
    "$$\n",
    "f(x)= \\frac{e^x - e^{-x}}{e^x + e^{-x}}\n",
    "$$\n",
    "\n",
    "или `логистическую функцию`:\n",
    "\n",
    "$$\n",
    "f(x)= \\frac{1}{1 + e^{-x}}\n",
    "$$\n",
    "\n",
    "Например, выберем логистическую функцию.\n",
    "\n",
    "Ее производная функции по аргументу $x$ дает очень простое выражение:\n",
    "\n",
    "$$\n",
    "f'(x)= f(x) \\cdot (1 - f(x))\n",
    "$$\n",
    "\n",
    "Именно его мы и запишем в нашу формулу вычисления локального градиента:\n",
    "$$\\delta = e \\cdot f'(v_{out}) = e \\cdot f(v_{out}) \\cdot (1 - f(v_{out})$$\n",
    "\n",
    "Но, так как\n",
    "$$y = f(v_{out})$$\n",
    "\n",
    "то локальный градиент последнего нейрона, равен:\n",
    "$$\\delta = e \\cdot y \\cdot (1 - y)$$\n",
    "\n",
    "Теперь у нас есть все, чтобы выполнить коррекцию весов. Начнем со связи $\\omega_{11}^{3}$, формула будет такой:\n",
    "$$\n",
    "\\begin{matrix}\n",
    "\\omega_{11}^{3} = \\omega_{11}^{3} - \\lambda \\cdot \\delta \\cdot f_{21} \\\\\n",
    "\\omega_{21}^{3} = \\omega_{11}^{3} - \\lambda \\cdot \\delta \\cdot f_{22}\n",
    "\\end{matrix}\n",
    "$$\n",
    "\n",
    "Здесь у вас может возникнуть вопрос: что такое параметр $\\lambda$ и где его брать? Он подбирается самостоятельно, вручную самим разработчиком. В самом простом случае можно попробовать следующие значения:\n",
    "$$\\lambda = 0.1, 0.01, 0.001, \\dots$$\n",
    "\n",
    "Итак, мы с вами скорректировали связи последнего слоя. Если вам все это понятно, значит, вы уже практически поняли весь алгоритм обучения, потому что дальше действуем подобным образом. Переходим к нейрону следующего с конца слоя и для его входящих связей повторим ту же самую процедуру. Но для этого нужно знать значение его локального градиента. Определяется он просто. Локальный градиент последнего нейрона взвешивается весами входящих в него связей. Полученные значения на каждом нейроне умножаются на производную функции активации, взятую в точках входной суммы:\n",
    "$$\n",
    "\\begin{matrix}\n",
    "\\delta_{21} = \\delta \\cdot \\omega_{11}^{3} \\cdot f'(v_{21}) = \\delta \\cdot \\omega_{11}^{3} \\cdot [f_{21} \\cdot (1 - f_{21})] \\\\\n",
    "\\delta_{22} = \\delta \\cdot \\omega_{21}^{3} \\cdot f'(v_{22}) = \\delta \\cdot \\omega_{21}^{3} \\cdot [f_{22} \\cdot (1 - f_{22})]\n",
    "\\end{matrix}\n",
    "$$\n",
    "\n",
    "А дальше действуем по такой же самой схеме, корректируем входные связи по той же формуле:\n",
    "$$\n",
    "\\begin{matrix}\n",
    "\\omega_{11}^{2} = \\omega_{11}^{2} - \\lambda \\cdot \\delta_{21} \\cdot f_{11} \\\\\n",
    "\\omega_{12}^{2} = \\omega_{12}^{2} - \\lambda \\cdot \\delta_{21} \\cdot f_{12} \\\\\n",
    "\\omega_{13}^{2} = \\omega_{13}^{2} - \\lambda \\cdot \\delta_{21} \\cdot f_{13}\n",
    "\\end{matrix}\n",
    "$$\n",
    "\n",
    "И для второго нейрона:\n",
    "$$\n",
    "\\begin{matrix}\n",
    "\\omega_{21}^{2} = \\omega_{21}^{2} - \\lambda \\cdot \\delta_{22} \\cdot f_{11} \\\\\n",
    "\\omega_{22}^{2} = \\omega_{22}^{2} - \\lambda \\cdot \\delta_{22} \\cdot f_{12} \\\\\n",
    "\\omega_{23}^{2} = \\omega_{23}^{2} - \\lambda \\cdot \\delta_{22} \\cdot f_{13}\n",
    "\\end{matrix}\n",
    "$$\n",
    "\n",
    "Осталось скорректировать веса первого слоя. Снова вычисляем локальные градиенты для нейронов первого слоя, но так как каждый из них имеет два выхода, то сначала вычисляем сумму от каждого выхода:\n",
    "$$\n",
    "\\begin{matrix}\n",
    "\\sigma_{11} = \\delta_{21} \\cdot \\omega_{11}^{2} + \\delta_{22} \\cdot \\omega_{21}^{2} \\\\\n",
    "\\sigma_{12} = \\delta_{21} \\cdot \\omega_{12}^{2} + \\delta_{22} \\cdot \\omega_{22}^{2} \\\\\n",
    "\\sigma_{13} = \\delta_{21} \\cdot \\omega_{13}^{2} + \\delta_{22} \\cdot \\omega_{23}^{2}\n",
    "\\end{matrix}\n",
    "$$\n",
    "\n",
    "А затем, значения локальных градиентов на нейронах первого скрытого слоя:\n",
    "$$\n",
    "\\begin{matrix}\n",
    "\\delta_{11} = \\sigma_{11} \\cdot f'(v_{11}) = \\sigma_{11} \\cdot [f_{11} \\cdot (1 - f_{11})] \\\\\n",
    "\\delta_{12} = \\sigma_{12} \\cdot f'(v_{12}) = \\sigma_{12} \\cdot [f_{12} \\cdot (1 - f_{12})] \\\\\n",
    "\\delta_{13} = \\sigma_{13} \\cdot f'(v_{13}) = \\sigma_{13} \\cdot [f_{13} \\cdot (1 - f_{13})]\n",
    "\\end{matrix}\n",
    "$$\n",
    "\n",
    "Ну и осталось выполнить коррекцию весов первого слоя все по той же формуле:\n",
    "$$\n",
    "\\begin{matrix}\n",
    "\\omega_{11}^{1} = \\omega_{11}^{1} - \\lambda \\cdot \\delta_{11} \\cdot x_1 \\\\\n",
    "\\omega_{12}^{1} = \\omega_{12}^{1} - \\lambda \\cdot \\delta_{11} \\cdot x_2 \\\\\n",
    "\\omega_{21}^{1} = \\omega_{21}^{1} - \\lambda \\cdot \\delta_{12} \\cdot x_1 \\\\\n",
    "\\omega_{22}^{1} = \\omega_{22}^{1} - \\lambda \\cdot \\delta_{12} \\cdot x_2 \\\\\n",
    "\\omega_{31}^{1} = \\omega_{31}^{1} - \\lambda \\cdot \\delta_{13} \\cdot x_1 \\\\\n",
    "\\omega_{32}^{1} = \\omega_{32}^{1} - \\lambda \\cdot \\delta_{13} \\cdot x_2\n",
    "\\end{matrix}\n",
    "$$\n",
    "\n",
    "В результате, мы выполнили одну итерацию алгоритма обучения НС. На следующей итерации мы должны взять другой входной вектор из нашего обучающего множества. Лучше всего это сделать случайным образом, чтобы не формировались возможные ложные закономерности в последовательности данных при обучении НС. Повторяя много раз этот процесс, весовые связи будут все точнее описывать обучающую выборку.\n",
    "\n",
    "Процесс обучения в целом мы рассмотрели. Но какой критерий качества минимизировался алгоритмом градиентного спуска? В действительности, мы стремились получить минимум суммы квадратов ошибок для обучающей выборки:\n",
    "$$\n",
    "E = \\frac{1}{2} \\cdot \\sum_{j=1}^{N} e_j^2 = \\frac{1}{2} \\cdot \\sum_{j=1}^{N} (d_j -y_j)^2\n",
    "$$\n",
    "\n",
    "То есть, с помощью алгоритма градиентного спуска веса корректируются так, чтобы минимизировать этот критерий качества работы НС."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4777d54a-1e45-4fc2-ab61-18364018b8a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
